{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tree_sitter import Language, Parser\n",
    "\n",
    "Language.build_library(\n",
    "    \".local/build/json_prefix.so\",\n",
    "    [\n",
    "        \"tree-sitter-json-prefix\"\n",
    "    ]\n",
    ")\n",
    "JSON_LANGUAGE = Language(\".local/build/json_prefix.so\", \"json_prefix\")\n",
    "parser = Parser()\n",
    "parser.set_language(JSON_LANGUAGE)\n",
    "\n",
    "def depth_first_traversal(node):\n",
    "    \"\"\"Depth-first traversal of the tree\"\"\"\n",
    "    yield node\n",
    "    for child in node.children:\n",
    "        yield from depth_first_traversal(child)\n",
    "\n",
    "def json_error_or_prefix(s):\n",
    "    tree = parser.parse(s)\n",
    "    nodes = [(node.has_error, node.type.startswith(\"prefix_\")) for node in depth_first_traversal(tree.root_node)]\n",
    "    error = any([node[0] for node in nodes])\n",
    "    prefix = any([node[1] for node in nodes])\n",
    "    return error, prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_load: loading model from '../llms/models/ggml-alpaca.bin' - please wait ...\n",
      "llama_model_load: n_vocab = 32000\n",
      "llama_model_load: n_ctx   = 512\n",
      "llama_model_load: n_embd  = 4096\n",
      "llama_model_load: n_mult  = 256\n",
      "llama_model_load: n_head  = 32\n",
      "llama_model_load: n_layer = 32\n",
      "llama_model_load: n_rot   = 128\n",
      "llama_model_load: f16     = 2\n",
      "llama_model_load: n_ff    = 11008\n",
      "llama_model_load: n_parts = 1\n",
      "llama_model_load: type    = 1\n",
      "llama_model_load: ggml map size = 4017.70 MB\n",
      "llama_model_load: ggml ctx size =  81.25 KB\n",
      "llama_model_load: mem required  = 5809.78 MB (+ 2052.00 MB per state)\n",
      "llama_model_load: loading tensors from '../llms/models/ggml-alpaca.bin'\n",
      "llama_model_load: model size =  4017.27 MB / num tensors = 291\n",
      "llama_init_from_file: kv self size  =  512.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "import llama_cpp\n",
    "\n",
    "MODEL_PATH = \"../llms/models/ggml-alpaca.bin\"\n",
    "\n",
    "llama = llama_cpp.Llama(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'j' 29926 22.213199615703903\n",
      "b'umps' 17204 26.740674972536617\n",
      "b'\",' 613 25.598178863533025\n",
      "b' \"' 376 24.236846923857915\n",
      "b'over' 957 22.420814514343288\n",
      "b'\",' 613 26.923004150392654\n",
      "b' \"' 376 23.873390197796752\n",
      "b'the' 1552 24.115888595614674\n",
      "b'\",' 613 23.48542785650847\n",
      "b' \"' 376 24.954296112075085\n",
      "b'la' 433 20.75459289647697\n",
      "b'zy' 1537 22.62041854873398\n",
      "b'\",' 613 24.2032699585269\n",
      "b' \"' 376 24.050636291539792\n",
      "b'dog' 26169 23.77336120610204\n",
      "b'\"]' 3108 24.41419982912651\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "n_vocab = llama_cpp.llama_n_vocab(llama.ctx)\n",
    "vocab = [llama_cpp.llama_token_to_str(llama.ctx, i) for i in range(n_vocab)]\n",
    "\n",
    "text = b'[\"the\", \"quick\", \"brown\", \"fox\", \"'\n",
    "\n",
    "tokens = llama.tokenize(text)\n",
    "llama.reset()\n",
    "\n",
    "completion = []\n",
    "\n",
    "n = 256\n",
    "for i in range(n):\n",
    "    # Eval\n",
    "    llama.eval(tokens)\n",
    "\n",
    "    # Sample\n",
    "    logits_raw = llama_cpp.llama_get_logits(llama.ctx)\n",
    "    logits = logits_raw[:n_vocab]\n",
    "    logprobs = [math.log(1.0 + math.exp(logit)) for logit in logits]\n",
    "    top_logprobs = sorted(zip(vocab, range(n_vocab), logprobs), key=lambda x: x[2], reverse=True)\n",
    "    tokens = None\n",
    "    for token, index, logprob in top_logprobs:\n",
    "        error, prefix = json_error_or_prefix(text + token)\n",
    "        if error:\n",
    "            continue\n",
    "\n",
    "        if prefix:\n",
    "            print(token, index, logprob)\n",
    "            completion.append((token, index, logprob))\n",
    "            tokens = [index]\n",
    "            break\n",
    "\n",
    "        print(token, index, logprob)\n",
    "        completion.append((token, index, logprob))\n",
    "        tokens = None\n",
    "        break\n",
    "    if tokens is None:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json.loads(text + b\"\".join([token for token, _, _ in completion]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
